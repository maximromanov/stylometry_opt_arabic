#d1862 <- read.delim(paste0(pathToFiles, "dispatch_1862.tsv"), encoding="UTF-8", header=TRUE, quote="")
sw1 <- scan(paste0(pathToFiles, "sw1.md"), what="character", sep="\n")
#install.packages("tidyverse", "readr", "stringr")
#install.packages("tidytext", "wordcloud", "RColorBrewer"", "quanteda", "readtext")
# General ones
library(tidyverse)
library(readr)
library("RColorBrewer")
# text analysis specific
library(stringr)
library(tidytext)
library(wordcloud)
library(quanteda)
library(readtext)
hypothenuse <- function(cathetus1, cathetus2) {
hypothenuse<- sqrt(cathetus1*cathetus1+cathetus2*cathetus2)
print(paste0("In the triangle with catheti of length ",
cathetus1, " and ", cathetus2, ", the length of hypothenuse is ", hypothenuse))
return(hypothenuse)
}
hypothenuse(390,456)
clean_up_text = function(x) {
x %>%
str_to_lower %>% # make text lower case
str_replace_all("[^[:alnum:]]", " ") %>% # remove non-alphanumeric symbols
str_replace_all("\\s+", " ") # collapse multiple spaces
}
text = "This is a sentence with punctuation, which mentions Vienna, the capital of Austria."
clean_up_text(text)
#install.packages("textstem")
library(textstem)
sentence = c("He tried to open one of the bigger boxes.", "The smaller boxes did not want to be opened.", "Different forms: open, opens, opened, opening, opened, opener, openers.")
str_split(sentence, "\\W+")
lemmatize_strings(sentence)
stem_strings(sentence)
library(tidytext)
d1862 <- read.delim(paste0(pathToFiles, "dispatch_1862.tsv"), encoding="UTF-8", header=TRUE, quote="")
head(d1862)
d1862 %>%
count(type, sort=T)
death_d1862 <- d1862 %>%
filter(type=="death" | type == "died")
test_set <- death_d1862
test_set_tidy <- test_set %>%
mutate(item_number = cumsum(str_detect(text, regex("^", ignore_case = TRUE)))) %>%
select(-type) %>%
unnest_tokens(word, text) %>%
mutate(word_number = row_number())
test_set_tidy
data("stop_words")
test_set_tidy_clean <- test_set_tidy %>%
anti_join(stop_words, by="word")
test_set_tidy_clean
test_set_tidy %>%
#anti_join(stop_words, by="word") %>%
count(word, sort = TRUE)
library(wordcloud)
library("RColorBrewer")
test_set_tidy_clean <- test_set_tidy %>%
anti_join(stop_words, by="word") %>%
count(word, sort=T)
set.seed(1234)
wordcloud(words=test_set_tidy_clean$word, freq=test_set_tidy_clean$n,
min.freq = 1, rot.per = .25, random.order=FALSE, #scale=c(5,.5),
max.words=150, colors=brewer.pal(8, "Dark2"))
# your code; your response
# your code; your response
# your code; your response
SW_to_DF <- function(path_to_file, episode){
sw_sentences <- scan(path_to_file, what="character", sep="\n")
sw_sentences <- as.character(sw_sentences)
sw_sentences <- gsub("([A-Z]) ([A-Z])", "\\1_\\2", sw_sentences)
sw_sentences <- gsub("([A-Z])-([A-Z])", "\\1_\\2", sw_sentences)
sw_sentences <- as.data.frame(cbind(episode, sw_sentences), stringsAsFactors=FALSE)
colnames(sw_sentences) <- c("episode", "sentences")
return(sw_sentences)
}
sw1_df <- SW_to_DF(paste0(pathToFiles, "sw1.md"), "sw1")
sw1_df_tidy <- sw1_df %>%
mutate(linenumber = row_number(),
chapter = cumsum(str_detect(sentences, regex("^#", ignore_case = TRUE))))
sw1_df_tidy <- sw1_df_tidy %>%
unnest_tokens(word, sentences)
ourWord = "sebulba"
word_occurance_vector <- which(sw1_df_tidy$word == ourWord)
#plot(x=word_occurance_vector, type="h", )
plot(0, type='n', #ann=FALSE,
xlim=c(1,length(sw1_df_tidy$word)), ylim=c(0,1),
main=paste0("Dispersion Plot of `", ourWord, "` in SW1"),
xlab="Movie Time", ylab=ourWord, yaxt="n")
segments(x0=word_occurance_vector, x1=word_occurance_vector, y0=0, y1=2)
# col=rgb(0,0,0,alpha=0.3) -- can be included as a parameter to segment to make lines more transparent
d1862 <- read.delim(paste0(pathToFiles, "dispatch_1862.tsv"), encoding="UTF-8", header=TRUE, quote="", stringsAsFactors = FALSE)
test_set <- d1862
test_set$date <- as.Date(test_set$date, format="%Y-%m-%d")
test_set_tidy <- test_set %>%
mutate(item_number = cumsum(str_detect(text, regex("^", ignore_case = TRUE)))) %>%
select(-type) %>%
unnest_tokens(word, text) %>%
mutate(word_number = row_number())
test_set_tidy
test_set_tidy_freqDay <- test_set_tidy %>%
anti_join(stop_words, by="word") %>%
group_by(date) %>%
count(word)
test_set_tidy_freqDay
# interesting examples:
# deserters, killed,
# donelson (The Battle of Fort Donelson took place in early February of 1862),
# manassas (place of the Second Bull Run, fought in August 28â€“30, 1862),
# shiloh (Battle of Shiloh took place in April of 1862)
ourWord = "manassas"
test_set_tidy_word <- test_set_tidy_freqDay %>%
filter(word==ourWord)
plot(x=test_set_tidy_word$date, y=test_set_tidy_word$n, type="l", lty=3, lwd=1,
main=paste0("Word `", ourWord, "` over time"),
xlab = "1862 - Dispatch coverage", ylab = "word frequency per day")
segments(x0=test_set_tidy_word$date, x1=test_set_tidy_word$date, y0=0, y1=test_set_tidy_word$n, lty=1, lwd=2)
library(quanteda)
library(readtext)
dispatch1862 <- readtext(paste0(pathToFiles, "dispatch_1862.tsv"), text_field = "text", quote="")
dispatch1862corpus <- corpus(dispatch1862)
kwic_test <- kwic(dispatch1862corpus, pattern = 'lincoln', window=10)
kwic_test
View(kwic_test)
knit_with_parameters('~/_OpenITI/RELEASE_Notes/release_notes_2020_1_2.Rmd')
knit_with_parameters('~/_OpenITI/RELEASE_Notes/release_notes_2020_1_2.Rmd')
install.packages("distill")
knit_with_parameters('~/_OpenITI/RELEASE_Notes/release_notes_2020_1_2_distill.Rmd')
library(readr)
OpenITI_metadata_2020_1_1 <- read_delim("_OpenITI/RELEASE_Notes/metadata/OpenITI_metadata_2020_1_1.csv",
"\t", escape_double = FALSE, trim_ws = TRUE)
# Loading metadata files from current and previous releases
library(readr)
cur <- read_delim("_OpenITI/RELEASE_Notes/metadata/OpenITI_metadata_2020_1_2.csv",
"\t", escape_double = FALSE, trim_ws = TRUE)
# Loading metadata files from current and previous releases
library(readr)
setwd("/Users/romanovienna/_OpenITI/RELEASE_Notes/")
cur <- read_delim("./metadata/OpenITI_metadata_2020_1_2.csv",
"\t", escape_double = FALSE, trim_ws = TRUE)
prv <- read_delim("./metadata/OpenITI_metadata_2020_1_1.csv",
"\t", escape_double = FALSE, trim_ws = TRUE)
# Loading metadata files from current and previous releases
library(readr)
setwd("/Users/romanovienna/_OpenITI/RELEASE_Notes/")
cur <- read_delim("./metadata/OpenITI_metadata_2020_1_2.csv",
"\t", escape_double = FALSE, trim_ws = TRUE)
prv <- read_delim("./metadata/OpenITI_metadata_2020_1_1.csv",
"\t", escape_double = FALSE, trim_ws = TRUE)
# Loading metadata files from current and previous releases
library(readr)
setwd("/Users/romanovienna/_OpenITI/RELEASE_Notes/")
cur <- read_delim("./metadata/OpenITI_metadata_2020_1_2.csv",
"\t", escape_double = FALSE, trim_ws = TRUE)
prv <- read_delim("./metadata/OpenITI_metadata_2020_1_1.csv",
"\t", escape_double = FALSE, trim_ws = TRUE)
# Loading metadata files from current and previous releases
library(readr)
setwd("/Users/romanovienna/_OpenITI/RELEASE_Notes/")
cur <- read_delim("./metadata/OpenITI_metadata_2020_1_2.csv",
"\t", escape_double = FALSE, trim_ws = TRUE)
prv <- read_delim("./metadata/OpenITI_metadata_2020_1_1.csv",
"\t", escape_double = FALSE, trim_ws = TRUE)
# Loading metadata files from current and previous releases
library(readr)
setwd("/Users/romanovienna/_OpenITI/RELEASE_Notes/")
cur <- read_delim("./metadata/OpenITI_metadata_2020_1_2.csv",
"\t", escape_double = FALSE, trim_ws = TRUE)
prv <- read_delim("./metadata/OpenITI_metadata_2020_1_1.csv",
"\t", escape_double = FALSE, trim_ws = TRUE)
# Loading metadata files from current and previous releases
library(readr)
setwd("/Users/romanovienna/_OpenITI/RELEASE_Notes/")
cur <- read_delim("./metadata/OpenITI_metadata_2020_1_2.csv",
"\t", escape_double = FALSE, trim_ws = TRUE)
prv <- read_delim("./metadata/OpenITI_metadata_2020_1_1.csv",
"\t", escape_double = FALSE, trim_ws = TRUE)
cur <- read_delim("./metadata/OpenITI_metadata_2020_1_2.csv",
"\t", escape_double = FALSE, trim_ws = TRUE)
prv <- read_delim("./metadata/OpenITI_metadata_2020_1_1.csv",
"\t", escape_double = FALSE, trim_ws = TRUE)
prv
prettyNum(nrow(cur), scientific=FALSE, big.mark=",")
# MINOR FUNCTIONS
bigNum <- function(number) {
return(prettyNum(number, scientific=FALSE, big.mark=","))
}
# MAIN LIBRARIES
library(tidyverse)
uniqueFiles <- unique(cur$versionUri)
uniqueTexts <- cur %>%
select(date, author, boob) %>%
unique
uniqueFiles <- unique(cur$versionUri)
uniqueTexts <- cur %>%
select(date, author, book) %>%
unique
uniqueFiles <- unique(cur$versionUri)
uniqueTexts <- cur %>%
select(date, author, book) %>%
unique
uniqueAuthors <- cur %>%
select(date, author) %>%
unique
uniqueFiles <- unique(cur$versionUri)
uniqueFiles <- unique(cur$versionUri)
uniqueTexts <- cur %>%
select(date, author, book) %>%
unique
uniqueAuthors <- cur %>%
select(date, author) %>%
unique
uniqueTexts
bigNum(nrow(uniqueTexts))
bigNum(nrow(uniqueAuthors))
bigNum(uniqueFiles)
uniqueFiles <- unique(cur)
uniqueTexts <- cur %>%
select(date, author, book) %>%
unique
uniqueAuthors <- cur %>%
select(date, author) %>%
unique
# Loading metadata files from current and previous releases
setwd("/Users/romanovienna/_OpenITI/RELEASE_Notes/")
cur <- read_delim("./metadata/OpenITI_metadata_2020_1_2.csv",
"\t", escape_double = FALSE, trim_ws = TRUE)
prv <- read_delim("./metadata/OpenITI_metadata_2020_1_1.csv",
"\t", escape_double = FALSE, trim_ws = TRUE)
head(cur)
# Loading metadata files from current and previous releases
setwd("/Users/romanovienna/_OpenITI/RELEASE_Notes/")
cur <- read_delim("./metadata/OpenITI_metadata_2020_1_2.csv",
"\t", escape_double = FALSE, trim_ws = TRUE)
prv <- read_delim("./metadata/OpenITI_metadata_2020_1_1.csv",
"\t", escape_double = FALSE, trim_ws = TRUE)
kable(head(cur))
install.packages("kableExtra")
tinytex::reinstall_tinytex()
# Loading metadata files from current and previous releases
setwd("/Users/romanovienna/_OpenITI/RELEASE_Notes/")
cur <- read_delim("./metadata/OpenITI_metadata_2020_1_2.csv",
"\t", escape_double = FALSE, trim_ws = TRUE)
prv <- read_delim("./metadata/OpenITI_metadata_2020_1_1.csv",
"\t", escape_double = FALSE, trim_ws = TRUE)
kable(head(cur))
kable(cur)
# MAIN LIBRARIES
library(tidyverse)
library(readr)
library(ggplot2)
library(knitr)
library(kableExtra)
kable(cur)
kable(head(cur))
head(cur) %>%
kable() %>%
kable_styling()
cur <- read_delim("./metadata/OpenITI_metadata_2020_1_2.csv",
"\t", escape_double = FALSE, trim_ws = TRUE)
#knitr::opts_chunk$set(message=FALSE, warning=FALSE, include=FALSE)
# MAIN LIBRARIES
library(tidyverse)
library(readr)
library(ggplot2)
library(knitr)
library(kableExtra)
# MINOR FUNCTIONS
bigNum <- function(number) {
return(prettyNum(number, scientific=FALSE, big.mark=","))
}
# Loading metadata files from current and previous releases
setwd("/Users/romanovienna/_OpenITI/RELEASE_Notes/")
cur <- read_delim("./metadata/OpenITI_metadata_2020_1_2.csv",
"\t", escape_double = FALSE, trim_ws = TRUE)
prv <- read_delim("./metadata/OpenITI_metadata_2020_1_1.csv",
"\t", escape_double = FALSE, trim_ws = TRUE)
cur
prv
#knitr::opts_chunk$set(message=FALSE, warning=FALSE, include=FALSE)
# MAIN LIBRARIES
library(tidyverse)
library(readr)
library(ggplot2)
library(knitr)
library(kableExtra)
# MINOR FUNCTIONS
bigNum <- function(number) {
return(prettyNum(number, scientific=FALSE, big.mark=","))
}
# Loading metadata files from current and previous releases
setwd("/Users/romanovienna/_OpenITI/RELEASE_Notes/")
cur <- read_delim("./metadata/OpenITI_metadata_2020_1_2.csv",
"\t", escape_double = FALSE, trim_ws = TRUE)
prv <- read_delim("./metadata/OpenITI_metadata_2020_1_1.csv",
"\t", escape_double = FALSE, trim_ws = TRUE)
library(tidyverse)
library(readr)
library(ggplot2)
library(knitr)
library(kableExtra)
bigNum <- function(number) {
return(prettyNum(number, scientific=FALSE, big.mark=","))
}
library(doParallel)
detectCores()
library(tidyverse)
library(ggplot2)
file <- "/Users/romanovienna/_ADHFANIS_Data/chapter_3/optimal_parameters/multiprocessing_results/_Hindawi_AllTexts_MultiProcessed_D20200517T160000.csv"
setwd("/Users/romanovienna/_ADHFANIS_Data/chapter_3/optimal_parameters/")
setwd("/Users/romanovienna/_ADHFANIS_Data/chapter_3/optimal_parameters/")
file <- "/multiprocessing_results/_Hindawi_AllTexts_MultiProcessed_D20200517T160000.csv"
data <- read_delim(file, "\t", escape_double = FALSE, col_names = FALSE, trim_ws = TRUE)
setwd("/Users/romanovienna/_ADHFANIS_Data/chapter_3/optimal_parameters/")
file <- "./multiprocessing_results/_Hindawi_AllTexts_MultiProcessed_D20200517T160000.csv"
data <- read_delim(file, "\t", escape_double = FALSE, col_names = FALSE, trim_ws = TRUE)
colnames(data) <- c("author_book", "clustering_method", "book_cluster_match",
"author_cluster_match", "distance_method", "dist_value",
"book_match", "book_dist_match", "author_dist_match",
"feature", "mff", "culling", "slices_compared", "slice_len")
View(data)
unique(data$distance_method)
unique(data$mff)
unique(data$culling)
unique(data$clustering_method)
for (distanceMethod in distanceMethodV){
for (clusteringMethod in clusteringMethodV){
for (featureType in featureTypeV){
for (nGramSize in nGramSizeV){
for (mffLimit in mffLimitV){
for (culling in cullingV){
# GRAPH CODE GOES HERE
message(distanceMethod,"::",clusteringMethod,"::",featureType,"::",nGramSize,"::",mffLimit,"::",culling)
}
}
}
}
}
}
# RUNNING PARAMETERS (HIGH-NUMBERS VECTORS)
prefixV            <- "HindawiSampleCorpus"
featureTypeV       <- c("w", "c") # c("c", "w")
nGramSizeV         <- c(1,2,3,4,5) #seq(1, 6, 1)
mffLimitV          <- seq(100, 500, 100)
cullingV           <- 0
samplingType       <- "slicing"
sliceStart         <- 1000 # means that the first `sliceStart` words will be skipped
sliceTotal         <- 2
sliceLengthV       <- seq(100, 11000, 100) #seq(100, 11000, 100)
# FORMATTED FOR EASY COMMENTING OUT OF SPECIFIC DISTANCES
distanceMethodV    <- c(# >>> STYLO DIST FUNCTIONS
"cosine",      # important
"delta",       # important
"argamon",
"eder",        # important
"minmax",      # important
"enthropy",
"simple",      # important
"wurzburg",    # important
# >>> DIST() FUNCTION
"maximum",
"manhattan",
"canberra",
"binary",
"minkowski",
"euclidean")
clusteringMethodV  <- c(# HCLUST METHODS
#"average",
#"median",
#"centroid",
"mcquitty",
#"complete",
"ward.D2"      # important
)
for (distanceMethod in distanceMethodV){
for (clusteringMethod in clusteringMethodV){
for (featureType in featureTypeV){
for (nGramSize in nGramSizeV){
for (mffLimit in mffLimitV){
for (culling in cullingV){
# GRAPH CODE GOES HERE
message(distanceMethod,"::",clusteringMethod,"::",featureType,"::",nGramSize,"::",mffLimit,"::",culling)
}
}
}
}
}
}
dataTemp <- data %>%
filter(distance_method="cosine", feature="w1", mff=300, culling=0)
dataTemp <- data %>%
filter(distance_method=="cosine", feature=="w1", mff==300, culling==0)
View(dataTemp)
dataTemp <- data %>%
filter(distance_method=="cosine", feature=="w1", mff==300, culling==0) %>%
group_by(slice_len) %>%
sum(book_cluster_match)
View(dataTemp)
dataTemp <- data %>%
filter(distance_method=="cosine", feature=="w1", mff==300, culling==0) %>%
group_by(slice_len)
dataTemp
dataTemp <- data %>%
filter(distance_method=="cosine", feature=="w1", mff==300, culling==0) %>%
group_by(slice_len) %>%
count(book_cluster_match)
View(dataTemp)
dataTemp <- data %>%
filter(distance_method=="cosine", feature=="w1", mff==300, culling==0) %>%
group_by(slice_len) %>%
sum(book_cluster_match)
dataTemp <- data %>%
filter(distance_method=="cosine", feature=="w1", mff==300, culling==0) %>%
group_by(slice_len) %>%
summarize(matches = sum(author_cluster_match))
View(dataTemp)
plot(dataTemp, x=slice_len, y=matches)
plot(dataTemp, x=dataTemp$slice_len, y=dataTemp$matches)
plot(dataTemp, x=dataTemp$slice_len, y=dataTemp$matches)
plot(dataTemp)
ggplot(dataTemp, aes(x=slice_len, y=matches))+
geom_line()
dataTemp <- data %>%
filter(distance_method=="cosine", feature=="w1", mff==300, culling==0) %>%
group_by(slice_len) %>%
summarize(matches = sum(book_cluster_match))
ggplot(dataTemp, aes(x=slice_len, y=matches))+
geom_line()
sys.frame()
sys.frame(1)
dirname(sys.frame(1)$ofile)
test <- dirname(sys.frame(1)$ofile)
help(scriptName)
??scriptName
?scriptName
# THE FOLLOWING EXTRACTS THE NAME OF THE CURRENT FILE< BUT WORKS ONLY WHEN RUN WITH `Rscript`
library(scriptName)
help(scriptName)
install.packages("funr")
system.getCurrentDirectory()
library(funr)
print(get_script_path())
setwd(""/Users/romanovienna/_ADHFANIS_Data/chapter_3/optimal_parameters"")
setwd("/Users/romanovienna/_ADHFANIS_Data/chapter_3/optimal_parameters")
getwd
getwd()
setwd("/Users/romanovienna/_ADHFANIS_Data/chapter_3/optimal_parameters/")
getwd()
projectPrefix <- "Hindawi_corpus65"
projectPrefix <- "Hindawi_corpus65"
resultsFolder <- paste0(projectPrefix,"_results_",format(Sys.time(), "D%Y%m%dT%H%M%S"))
resultsFolder
hindawi65 <- "/Users/romanovienna/_ADHFANIS_Data/chapter_3/optimal_parameters/Hindawi_Corpus_65/"
hindawi65files <- unlist(list.files(hindawi65))
hindawi65files
hindawi65df <- as.data.frame(hindawi65files)
hindawi65df
hindawi65df <- as.data.frame(hindawi65files)
hindawi65df <- hindawi65fd %>%
mutate(authors = str_replace_all(hindawi65files, "_[\\w\\.]+$", "")) %>%
group_by(author) %>%
count()
#library
hindawi65df <- hindawi65df %>%
mutate(authors = str_replace_all(hindawi65files, "_[\\w\\.]+$", "")) %>%
group_by(author) %>%
count()
hindawi65df <- hindawi65df %>%
mutate(authors = str_replace_all(hindawi65files, "_[\\w\\.]+$", "")) %>%
group_by(authors) %>%
count()
View(hindawi65df)
hindawi65df
corpusFolder  <- "./Hindawi_Corpus65/"
projectPrefix <- str_replace(corpusFolder, "\\W+", "")
projectPrefix
corpusFolder  <- "./Hindawi_Corpus65/"
projectPrefix <- str_replace_all(corpusFolder, "\\W+", "")
projectPrefix
projectPrefix <- str_replace_all(corpusFolder, "\\W+", "")
resultsFolder <- paste0(projectPrefix,"_results_",format(Sys.time(), "D%Y%m%dT%H%M%S"))
savePath      <- paste0("./",resultsFolder,"/")
projectPrefix
resultsFolder
savePath
# FULL LIST
listOfFiles <- list.files(corpusFolder)
listOfFiles
??str_replace_all
?str_replace_all
message("STARTING MULTIPROCESSING...\n\t%s CORES" % clusterCores)
message("STARTING MULTIPROCESSING...\n\t",clusterCores," CORES")
"="*80
length(seq(10,3000,10))
length(seq(100,11000,100))
length(seq(20,3000,20))
?read_delim
# REMOVE RESULTS ONLY AFTER ALL OTHER STEPS ARE COMPLETE
remove.temp.results(results.dir = savePath, prefix = "_TEMP_")
as.integer(8/3*2)
setwd("/Users/romanovienna/_ADHFANIS_Data/chapter_3/stylometry_opt_arabic/")
file <- "/Users/romanovienna/_ADHFANIS_Data/chapter_3/arc/arc_testing_halves_now_optimal_parameters/multiprocessing_results/_Hindawi_AllTexts_MultiProcessed_D20200517T160000.csv"
data <- read_delim(file, "\t", escape_double = FALSE, col_names = FALSE, trim_ws = TRUE)
colnames(data) <- c("author_book", "clustering_method", "book_cluster_match",
"author_cluster_match", "distance_method", "dist_value",
"book_match", "book_dist_match", "author_dist_match",
"feature", "mff", "culling", "slices_compared", "slice_len")
unique(data$distance_method)
unique(data$mff)
unique(data$culling)
unique(data$clustering_method)
